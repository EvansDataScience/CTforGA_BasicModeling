<br> 
<center><img src="http://i.imgur.com/sSaOozN.png" width="500"></center>


## Course:  COMPUTATIONAL THINKING FOR GOVERNANCE ANALYTICS

### Prof. José Manuel Magallanes, PhD 
* Visiting Professor of Computational Policy at Evans School of Public Policy and Governance, and eScience Institute Senior Data Science Fellow, University of Washington.
* Professor of Government and Political Methodology, Pontificia Universidad Católica del Perú. 


_____
<a id='part1'></a>

# Basic Modeling in R


Collect the data about democracy that we prepared in Python:

```{r, eval=TRUE}
# clean memory
rm(list = ls())

#link To file
link='https://github.com/EvansDataScience/CTforGA_DimensionReduction/raw/main/fromPyPlus.RDS'
# a RDS file from the web needs:
myFile=url(link)

# reading in data:
multidata=readRDS(file = myFile)

# reset indexes to R format:
row.names(multidata)=NULL
```


Verifying data structure:

```{r, eval=TRUE}
str(multidata,width = 70,strict.width='cut')
```




# <font color="red">R for Regression</font>

## Continuous outcome 

Generally speaking, we use regression when we have a continuous value as the dependent variable, and a set of independent variables (of different types). 

### * __EXPLANATORY APPROACH__

1. State hypotheses:

Prepare your hypotheses:
```{r, eval=TRUE}
# hypothesis 1: democracy is related to the level of security apparatus:
hypo1=formula(Demo_efa~ C1SecurityApparatus)

# hypothesis 2: democracy is related to the levels of security apparatus, and economic inequality:

hypo2=formula(Demo_efa~ C1SecurityApparatus + P1StateLegitimacy)

# hypothesis 3: democracy is related to the levels of security apparatus, economic inequality, and human rights:

hypo3=formula(Demo_efa~ C1SecurityApparatus + P1StateLegitimacy + P3HumanRights)
```


2. Compute regression models:

```{r, eval=TRUE}
#
# results
gauss1=glm(hypo1,
           data = multidata,
           family = 'gaussian')

gauss2=glm(hypo2,
           data = multidata,
           family = 'gaussian')

gauss3=glm(hypo3,
           data = multidata,
           family = 'gaussian')
```

3. See results:

* Test of first Hypothesis

```{r, eval=TRUE}
summary(gauss1)
```

* Test of second Hypothesis

```{r, eval=TRUE}
summary(gauss2)
```
* Test of third Hypothesis:

```{r, eval=TRUE}
summary(gauss3)
```
You may want to see the R-squared values for each regression:
```{r, eval=TRUE}
library(rsq)
rsq(gauss1,adj=T); rsq(gauss2,adj=T); rsq(gauss3,adj=T)
```
You the the last to ones have a better R-squared value than the first one. The question remains if the third is significantly better than the second one.

4. Looking for _better_ model:


```{r, eval=TRUE}
anova(gauss1,gauss2,gauss3,test="Chisq")
```

The second model is better than the first one; however, the third is significantly better than the previous two. You should keep the third one.



5. Verify the situation of chosen model:

5.1. Linearity between dependent variable and predictors is assumed, then these dots should follow a linear and horizontal trend:

```{r, eval=TRUE}
plot(gauss3,1)
```


5.2. Normality of residuals is assumed:

Visual exploration:
```{r, eval=TRUE}
plot(gauss3,2)
```

Mathematical exploration:
```{r, eval=TRUE}
# normality holds if the p-value is above 0.05
shapiro.test(gauss3$residuals)
```

5.3. Homoscedasticity is assumed, so you need to check if residuals are spread equally along the ranges of predictors:

Visual exploration:

```{r, eval=TRUE}
plot(gauss3, 3)
```

Mathematical exploration:
```{r, eval=TRUE}
library(lmtest)
#if pvalue<0.05 you cannot assume Homoscedasticity
bptest(gauss3) 
```

5.4. We assume that there is no colinearity, that is, that the predictors are not highly correlated.


```{r, eval=TRUE}
library(car)
vif(gauss3) # lower than 5 is desirable
```

5.5. Analize the effect of atypical values. Determine if outliers (points that are far from the rest, but still in the trend) or high-leverage points (far from the trend) are influential:

Visual exploration:
```{r, eval=TRUE}
plot(gauss3,5)
```

Querying:
```{r, eval=TRUE}
gaussInf=as.data.frame(influence.measures(gauss3)$is.inf)
gaussInf[gaussInf$cook.d,]
```


6. Finally, a nice summary plot of your work:

```{r, eval=TRUE}
library(sjPlot)

plot_models(gauss1,gauss2,gauss3,vline.color = "grey")
```


* __PREDICTIVE APPROACH__


1. Split the data set:

```{r, eval=TRUE}
library(caret)

set.seed(123)

# selects some rows
selection = createDataPartition(multidata$Demo_efa,
                                p = 0.75,
                                list = FALSE)
# keep only selected
trainGauss = multidata[ selection, ]

#keep all but selected
testGauss  = multidata[-selection, ]
```

2. Regress with train data

Let's use **cross validation**: applying the regression to several samples  (here 5) from the training data set:

```{r, eval=TRUE}
ctrl = trainControl(method = 'cv', #cross validation
                    number = 5)

gauss3CV = train(hypo3,
                 data = trainGauss, 
                 method = 'glm',
                 trControl = ctrl)

gauss3CV

```
Just checking the Rsquared you find a value better than 0.5, which is good enough.

3. Evaluate performance


```{r, eval=TRUE}

predictedVal<-predict(gauss3CV,testGauss)

postResample(obs = testGauss$Demo_efa,
             pred=predictedVal)
```
From the information above, you find a good Rsquared.


## Binary outcome 

In this situation you have binary dependent variable, which we do not currently have, let's create one:

```{r, eval=TRUE}
multidata$Demo_dico=ifelse(multidata$Demo_efa>median(multidata$Demo_efa,
                                                   na.rm = T),
                         yes=1,
                         no=0)
```

 

### * __EXPLANATORY APPROACH__

1. State hypothesis:

Let's use the same ones:

```{r, eval=TRUE}
hypoDico1=formula(Demo_dico~ C1SecurityApparatus)
hypoDico2=formula(Demo_dico~ C1SecurityApparatus + P1StateLegitimacy)
hypoDico3=formula(Demo_dico~ C1SecurityApparatus + P1StateLegitimacy + P3HumanRights)

```

2. Reformat

```{r, eval=TRUE}
multidata$Demo_dico=factor(multidata$Demo_dico)
```


6. Compute regression models:

```{r, eval=TRUE}
Logi1=glm(hypoDico1,data = multidata,
          family = "binomial")
Logi2=glm(hypoDico2,data = multidata,
          family = "binomial")
Logi3=glm(hypoDico3,data = multidata,
          family = "binomial")
```

7. See results:

* First Hypothesis test:
```{r, eval=TRUE}
summary(Logi1)
```
* Second Hypothesis test:

```{r, eval=TRUE}
summary(Logi2)
```

* Third Hypothesis test:

```{r, eval=TRUE}
summary(Logi3)
```

8. Search for better model:
```{r, eval=TRUE}
lrtest(Logi1,Logi2,Logi3)
```

The third hypothesis would hold again.


9. Verify the situation of chosen model

9.1. Linearity assumption (Box-Tidwell test)

```{r, eval=TRUE}

multidata$secuTEST=multidata$C1SecurityApparatus*log(multidata$C1SecurityApparatus)
multidata$legitTEST=multidata$P1StateLegitimacy*log(multidata$P1StateLegitimacy)
multidata$humanTEST=multidata$P3HumanRights*log(multidata$P3HumanRights)

DicoTest=formula(Demo_dico~ C1SecurityApparatus + P1StateLegitimacy + P3HumanRights + secuTEST + legitTEST + humanTEST)

summary(glm(DicoTest,data=multidata,family = binomial))

```
From the table above, your main concern should be the significant TEST variables.


9.2. We assume that there is no colinearity, that is, that the predictors are not correlated.

```{r, eval=TRUE}
vif(Logi3)
```

9.3 Analize the effect of atypical values. Determine if outliers (points that are far from the rest, but still in the trend) or high-leverage points (far from the trend but close to the rest) are influential:

Visual exploration:

```{r, eval=TRUE}
plot(Logi3,5)
```

10. Finally, a nice summary plot of your work by computing the marginal effects:

```{r, eval=TRUE}
library(margins)
(modelChosen = margins(Logi3))
```

```{r, eval=TRUE}
(margins=summary(modelChosen))
```


```{r, eval=TRUE}

base= ggplot(margins,aes(x=factor, y=AME)) + geom_point()
plot2 = base + theme(axis.text.x = element_text(angle = 80,
                                              size = 6,
                                              hjust = 1))
plot2    
```
```{r, eval=TRUE}
plot2 +  geom_errorbar(aes(ymin=lower, ymax=upper))
```

### * __PREDICTIVE APPROACH__


1. Split the data set:
```{r, eval=TRUE}
set.seed(123)

selection = createDataPartition(multidata$Demo_dico,
                                p = 0.75,
                                list = FALSE)
trainLogi = multidata[selection, ]
testLogi  = multidata[-selection, ]
```

2. Regress with train data

Let’s use cross validation, applying the regression to five samples from the training data set:
```{r, eval=TRUE}
set.seed(123)
ctrl = trainControl(method = 'cv',number = 5)

Logis3CV = train(hypoDico3,
                 data = trainLogi, 
                 method = 'glm',
                 family="binomial",
                 trControl = ctrl)
```

3. See results:

```{r, eval=TRUE}
Logis3CV
```

3. Evaluate performance

3.1 Get predictions on test data
```{r, eval=TRUE}

predictions = predict(Logis3CV,
                      newdata=testLogi,
                      type='raw')
```

3.2 Assess performance 
```{r, eval=TRUE}
confusionMatrix(data=predictions,
                reference=testLogi$Demo_dico,
                positive = "1")
```

Here is some help for you to interpret the [result](https://docs.google.com/presentation/d/e/2PACX-1vRyFvA5U2HXC3CGBeKEr209_KLjjdeo8FoKT6BxMsjLNOJwx7YDUmGEpDq1pzfGO7Zizuk5rogUNPgj/pub?start=false&loop=false&delayms=3000).



